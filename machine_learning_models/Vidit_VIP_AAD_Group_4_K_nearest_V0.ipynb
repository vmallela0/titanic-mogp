{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNrHzRP5WQ0c"
      },
      "source": [
        "Make sure to put the train.csv and test.csv in a folder called data in the home directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5AUTfZyLk1g"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkb0TFGTUWEs"
      },
      "source": [
        "Ankith: Neural network\n",
        "\n",
        "Jenny: XGBoost\n",
        "\n",
        "Vedu: Random Forest\n",
        "\n",
        "Ethan: Support Vector Machines\n",
        "\n",
        "Vidit: k-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "rhxjBq04UTEo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "bj2Z4PTxVZwa",
        "outputId": "6f9bfa58-4920-425c-d930-1a028838cdc0"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../data/train.csv')\n",
        "test = pd.read_csv('../data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "MUVzwXTtWjlY"
      },
      "outputs": [],
      "source": [
        "train['set'], test['set'] = 'train', 'test'\n",
        "combined = pd.concat([train, test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "uusl22JqbkjE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PassengerId       0\n",
              "Survived        418\n",
              "Pclass            0\n",
              "Name              0\n",
              "Sex               0\n",
              "Age             263\n",
              "SibSp             0\n",
              "Parch             0\n",
              "Ticket            0\n",
              "Fare              1\n",
              "Cabin          1014\n",
              "Embarked          2\n",
              "set               0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ZgE8WqKlbovS"
      },
      "outputs": [],
      "source": [
        "pclass = combined.loc[combined.Fare.isnull(), 'Pclass'].values[0]\n",
        "median_fare = combined.loc[combined.Pclass== pclass, 'Fare'].median()\n",
        "combined.loc[combined.Fare.isnull(), 'Fare'] = median_fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "M78GVtlGbtHO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n",
              "       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess',\n",
              "       'Jonkheer', 'Dona'], dtype=object)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined['Title'] = combined['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
        "combined['Title'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "loEWgdP5bvwf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Mr', 'Mrs', 'Miss', 'Master', 'Rev', 'Dr'], dtype=object)"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "title_reduction = {'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss',\n",
        "                   'Master': 'Master', 'Don': 'Mr', 'Rev': 'Rev',\n",
        "                   'Dr': 'Dr', 'Mme': 'Miss', 'Ms': 'Miss',\n",
        "                   'Major': 'Mr', 'Lady': 'Mrs', 'Sir': 'Mr',\n",
        "                   'Mlle': 'Miss', 'Col': 'Mr', 'Capt': 'Mr',\n",
        "                   'Countess': 'Mrs','Jonkheer': 'Mr',\n",
        "                   'Dona': 'Mrs'}\n",
        "combined['Title'] = combined['Title'].map(title_reduction)\n",
        "combined['Title'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "t67H02_5b1ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr 49.0\n",
            "Master 4.0\n",
            "Miss 22.0\n",
            "Mr 30.0\n",
            "Mrs 36.0\n",
            "Rev 41.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_22196\\1042440295.py:1: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for title, age in combined.groupby('Title')['Age'].median().iteritems():\n"
          ]
        }
      ],
      "source": [
        "for title, age in combined.groupby('Title')['Age'].median().iteritems():\n",
        "    print(title, age)\n",
        "    combined.loc[(combined['Title']==title) & (combined['Age'].isnull()), 'Age'] = age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "uYdi-d-kb4xl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PassengerId       0\n",
              "Survived        418\n",
              "Pclass            0\n",
              "Name              0\n",
              "Sex               0\n",
              "Age               0\n",
              "SibSp             0\n",
              "Parch             0\n",
              "Ticket            0\n",
              "Fare              0\n",
              "Cabin          1014\n",
              "Embarked          2\n",
              "set               0\n",
              "Title             0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "lVRsZWPMb43k"
      },
      "outputs": [],
      "source": [
        "def other_family_members_survived(dataset, label='family_survival'):\n",
        "    \"\"\"\n",
        "    Check if other family members survived\n",
        "      -> 0 other did not survive\n",
        "      -> 1 at least one other family member survived\n",
        "      -> 0.5 unknown if other members survived or person was alone\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : DataFrame\n",
        "      The sub-dataframe containing the family\n",
        "    \"\"\"\n",
        "    ds = dataset.copy()\n",
        "    if len(dataset) == 1:\n",
        "        ds[label] = 0.5\n",
        "        return ds\n",
        "    result = []\n",
        "    for ix, row in dataset.iterrows():\n",
        "        survived_fraction = dataset.drop(ix)['Survived'].mean()\n",
        "        if np.isnan(survived_fraction):\n",
        "            result.append(0.5)\n",
        "        elif survived_fraction == 0:\n",
        "            result.append(0)\n",
        "        else:\n",
        "            result.append(1)\n",
        "    ds[label] = result\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Ha0pGfqOcL9D"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_22196\\2887954477.py:2: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  combined = combined.groupby(['surname', 'Fare']).apply(other_family_members_survived).reset_index(drop=True)\n"
          ]
        }
      ],
      "source": [
        "combined['surname'] = combined['Name'].apply(lambda x: x.split(\",\")[0])\n",
        "combined = combined.groupby(['surname', 'Fare']).apply(other_family_members_survived).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "dVf4_Q8XcMkh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_22196\\2452194539.py:1: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  combined = combined.groupby(['Ticket']).apply(lambda x: other_family_members_survived(x, label='family_survival_ticket')).reset_index(drop=True)\n"
          ]
        }
      ],
      "source": [
        "combined = combined.groupby(['Ticket']).apply(lambda x: other_family_members_survived(x, label='family_survival_ticket')).reset_index(drop=True)\n",
        "combined.loc[combined['family_survival'] == 0.5, 'family_survival'] = combined.loc[combined['family_survival'] == 0.5, 'family_survival_ticket']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "77Au8wg-cO03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_22196\\2024603283.py:3: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  combined.loc[:, 'Age'] = pd.qcut(combined['Age'], 4, labels=False)\n",
            "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_22196\\2024603283.py:4: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  combined.loc[:, 'Fare'] = pd.qcut(combined['Fare'], 5, labels=False)\n"
          ]
        }
      ],
      "source": [
        "combined['family_size'] = combined['Parch'] + combined['SibSp']\n",
        "combined['Sex'] = LabelEncoder().fit_transform(combined['Sex'])\n",
        "combined.loc[:, 'Age'] = pd.qcut(combined['Age'], 4, labels=False)\n",
        "combined.loc[:, 'Fare'] = pd.qcut(combined['Fare'], 5, labels=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "2ZwgN5PzcXgu"
      },
      "outputs": [],
      "source": [
        "selected = ['Pclass', 'Sex', 'Age', 'Fare', 'family_size', 'family_survival']\n",
        "scaler  = StandardScaler()\n",
        "scaler.fit(combined[selected])\n",
        "combined[selected] = scaler.transform(combined[selected])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ePMXWv_ZcdGT"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ankit\\OneDrive\\Documents\\Georgia Tech\\Automated Alg Design\\VIP_AAD_Group_Project_4\\titanic-mogp\\machine_learning_models\\Vidit_VIP_AAD_Group_4_K_nearest_V0.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ankit/OneDrive/Documents/Georgia%20Tech/Automated%20Alg%20Design/VIP_AAD_Group_Project_4/titanic-mogp/machine_learning_models/Vidit_VIP_AAD_Group_4_K_nearest_V0.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m combined\u001b[39m.\u001b[39;49mto_parquet(\u001b[39m'\u001b[39;49m\u001b[39mtitanic_family_survivabillity.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:2975\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2888\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2889\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2972\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2975\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2976\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2977\u001b[0m     path,\n\u001b[0;32m   2978\u001b[0m     engine,\n\u001b[0;32m   2979\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m   2980\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m   2981\u001b[0m     partition_cols\u001b[39m=\u001b[39mpartition_cols,\n\u001b[0;32m   2982\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   2983\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2984\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parquet.py:426\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(partition_cols, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    425\u001b[0m     partition_cols \u001b[39m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 426\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m    428\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m    430\u001b[0m impl\u001b[39m.\u001b[39mwrite(\n\u001b[0;32m    431\u001b[0m     df,\n\u001b[0;32m    432\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    438\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parquet.py:52\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     50\u001b[0m             error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(err)\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to find a usable engine; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtried using: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfastparquet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA suitable version of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupport.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to import the above resulted in these errors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merror_msgs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m PyArrowImpl()\n",
            "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
          ]
        }
      ],
      "source": [
        "combined.to_parquet('titanic_family_survivabillity.parquet', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Mw_aNqceqE"
      },
      "outputs": [],
      "source": [
        "train = combined.loc[combined['set'] == 'train'].drop('set', axis=1).reset_index(drop=True)\n",
        "test = combined.loc[combined['set'] == 'test'].drop(['set', 'Survived'], axis=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xowf94PMoE3"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vector1, vector2):\n",
        "    return np.sqrt(np.sum((vector1 - vector2)**2))\n",
        "\n",
        "# test function\n",
        "vec1 = np.array([3, 0])\n",
        "vec2 = np.array([0, 4])\n",
        "\n",
        "# this is the 3:4:5 triangle and therefore, it should return 5 (Long live Pythagoras)\n",
        "euclidean_distance(vec1, vec2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbHHDmaCcgOF"
      },
      "outputs": [],
      "source": [
        "def get_nearest_neighbor(vector, dataset, number_of_neighbors=1, ignore_cols=['Survived']):\n",
        "    distances = []\n",
        "    for ix, row in dataset.loc[:, ~dataset.columns.isin(ignore_cols)].iterrows():\n",
        "        distance = euclidean_distance(row, vector)\n",
        "        distances.append((distance, ix))\n",
        "    indices = [x[1] for x in sorted(distances, key=lambda x: x[0])]\n",
        "    neighbors = dataset.loc[indices[:number_of_neighbors]]\n",
        "    return neighbors\n",
        "\n",
        "# Another implementation using Pandas\n",
        "def get_nearest_neighbor(vector, dataset, number_of_vectors=1, ignore_cols=['Survived'], not_count_duplicates=False):\n",
        "    ds = dataset.copy()\n",
        "    ds['distance'] = ds.loc[:, ~ds.columns.isin(ignore_cols)].apply(\n",
        "        lambda x: euclidean_distance(x, vector), axis=1)\n",
        "    if not_count_duplicates:\n",
        "        distances = sorted(ds.distance.unique())[:number_of_vectors]\n",
        "        return ds.loc[ds.distance <= max(distances)].drop('distance', axis=1)\n",
        "    return ds.sort_values('distance', ascending=True).head(number_of_vectors).drop('distance', axis=1)\n",
        "\n",
        "# test function\n",
        "dataset = pd.DataFrame([\n",
        "    {'a': 1, 'b': 1, 'Survived': 1},\n",
        "    {'a': 2, 'b': 2, 'Survived': 1},\n",
        "    {'a': 3, 'b': 3, 'Survived': 0},\n",
        "    {'a': 4, 'b': 4, 'Survived': 0},\n",
        "    {'a': 5, 'b': 5, 'Survived': 0},\n",
        "])\n",
        "vector = pd.Series({'a': 2.5, 'b': 2.5})\n",
        "\n",
        "# should be (2,2) and (3,3) (if keeping track of duplicates)\n",
        "get_nearest_neighbor(vector, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHBlQrC_ch6F"
      },
      "outputs": [],
      "source": [
        "def predict(vector, dataset, number_of_neighbors=1, y='Survived'):\n",
        "    neighbors = get_nearest_neighbor(vector, dataset, number_of_neighbors)\n",
        "    return round(neighbors[y].mean())\n",
        "\n",
        "# test function\n",
        "print(predict(vector, dataset))\n",
        "print(predict(pd.Series({'a': 4.5, 'b': 4.5}), dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLpDRbzqc4op"
      },
      "outputs": [],
      "source": [
        "def predict_dataset(dataset, number_of_neighbors=1):\n",
        "    ds = dataset.copy()\n",
        "    def predict_row(vector, dataset):\n",
        "        subset = dataset.loc[~(dataset.index==vector.name)]\n",
        "        if vector.name % 100 == 0:\n",
        "            print(vector.name)\n",
        "        return int(predict(vector, subset, number_of_neighbors))\n",
        "\n",
        "    ds['predicted'] = ds.loc[:, ds.columns.isin(selected)].apply(\n",
        "        lambda x: predict_row(x, ds), axis=1)\n",
        "\n",
        "    return ds\n",
        "\n",
        "ds = predict_dataset(train, number_of_neighbors=10)\n",
        "\n",
        "print('Accuracy:', sum(ds['Survived'] == ds['predicted']) / len(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yZfV3cGm9zr"
      },
      "outputs": [],
      "source": [
        "def predict_testset(test_dataset, train_dataset, number_of_neighbors=1):\n",
        "    ds = test_dataset.copy()\n",
        "    select = selected + ['Survived']\n",
        "\n",
        "    def predict_row(vector, dataset):\n",
        "        if vector.name % 100 == 0:\n",
        "            print(vector.name)\n",
        "        return int(predict(vector, dataset[select], number_of_neighbors))\n",
        "\n",
        "    ds['Survived'] = ds.loc[:, ds.columns.isin(selected)].apply(\n",
        "        lambda x: predict_row(x, train_dataset), axis=1)\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doGob9_PqNlU"
      },
      "outputs": [],
      "source": [
        "random.seed(12)\n",
        "np.random.seed(12)\n",
        "MLP_cls = MLPClassifier(hidden_layer_sizes=(10,10,10,10), max_iter = 200)\n",
        "MLP_cls.fit(X_train,y_train)\n",
        "y_pred = MLP_cls.predict(X_test)\n",
        "final_test = predict_testset(test, train, number_of_neighbors=10)\n",
        "result = final_test[['PassengerId', 'Survived']].copy()\n",
        "result\n",
        "result.to_csv('titanic_prediction_group_4_vidit.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a_j8obXo4v-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(y_test,svm_clf.predict(X_test))\n",
        "print(confusion_matrix)\n",
        "FNR = confusion_matrix[0][1] / (confusion_matrix[0][0] + confusion_matrix[0][1])\n",
        "FPR = confusion_matrix[1][1] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n",
        "print(FNR,FPR)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
